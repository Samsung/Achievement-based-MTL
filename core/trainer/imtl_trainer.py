from .trainer import Trainer
from ..lr_scheduler import build_scheduler
from ..optimizer.optimizer import build_optimizer_from_params


class IMTLTrainer(Trainer):
    def __init__(self, args, net, data_loader, teachers):
        super(IMTLTrainer, self).__init__(args, net, data_loader, teachers)

    def _backward(self, loss, scaler, epoch=None):
        shared_loss, task_loss = loss
        self.optimizer.zero_grad(set_to_none=True)
        self.task_optimizer.zero_grad(set_to_none=True)

        # propagate gradient by shared loss, alpha_t * L_t
        scaler.scale(shared_loss).backward(retain_graph=True)

        # Keep gradients for shared parameters
        shared_grad_group = []
        for param_group in self.optimizer.param_groups:
            shared_grad_group.append([param.grad.detach().clone() for param in param_group['params']])

        # propagate gradient by task loss, (1 - alpha_t) * L_t
        scaler.scale(task_loss).backward()

        # for task-specific parameters, the gradients are generated by (1 - alpha_t) * L_t + alpha_t * L_t = L_t
        scaler.step(self.task_optimizer)

        # restore gradients for shared parameters
        for shared_grads, param_group in zip(shared_grad_group, self.optimizer.param_groups):
            for shared_grad, param in zip(shared_grads, param_group['params']):
                param.grad = shared_grad

        scaler.step(self.optimizer)
        scaler.update()

        self.scheduler.step(epoch)
        self.task_scheduler.step(epoch)

    def _log(self, epoch, batch_iter, timer):
        super(IMTLTrainer, self)._log(epoch, batch_iter, timer)
        self.print(str(self.loss_function.loss_scale))

    def _get_params(self, net, lf):
        shared_params_decay, shared_params_no_decay, task_params_decay, task_params_no_decay = [], [], [], []
        for name, param in net.named_parameters():
            if 'backbone' in name or 'transition' in name or 'FPN' in name \
                    or 'shared_aspp' in name or 'predict.skip_conv' in name:
                self.print(f'{name} is shared parameter')
                shared_params_decay.append(param)
            else:
                self.print(f'{name} is task-specific parameter')
                task_params_decay.append(param)

        task_params_decay += [value for key, value in lf.loss_scale.items()]
        return [{'params': shared_params_no_decay, 'weight_decay': 0.0}, {'params': shared_params_decay}], \
               [{'params': task_params_no_decay, 'weight_decay': 0.0}, {'params': task_params_decay}]

    def _build_optimizer(self, args):
        net_without_ddp = self.net.module if hasattr(self.net, 'module') else self.net
        shared_params, task_params = self._get_params(net_without_ddp, self.loss_function)
        self.optimizer = build_optimizer_from_params(args, shared_params)
        self.task_optimizer = build_optimizer_from_params(args, task_params)
        return self.optimizer

    def _build_scheduler(self, args):
        self.task_scheduler = build_scheduler(args, self.task_optimizer)
        return super(IMTLTrainer, self)._build_scheduler(args)

    def update_scheduler(self, epoch, metrics=None):
        self.task_scheduler.step(epoch=epoch, metrics=metrics)
        super(IMTLTrainer, self).update_scheduler(epoch, metrics)
